{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhild2710/Game-revenue-predictions/blob/main/Games_Revenue_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "190eb4d2",
      "metadata": {
        "id": "190eb4d2"
      },
      "source": [
        "\n",
        "# Paid PC Games — Revenue Prediction\n",
        "\n",
        "This notebook trains a machine learning model to predict the revenue of paid PC games based on publicly available metadata.  \n",
        "\n",
        "\n",
        "**Key points:**\n",
        "- Input: Game metadata such as price, reviews, review score, peak concurrent players, and publisher class.  \n",
        "- Output: Estimated revenue per game.  \n",
        "- The dataset used for training is private; you can run the notebook with your own dataset following the same column schema.\n",
        "\n",
        "**Expected Input Columns**:\n",
        "- `price`\n",
        "- `total_reviews`\n",
        "- `total_positive`\n",
        "- `total_negative`\n",
        "- `reviewScore`\n",
        "- `peak_all_time`\n",
        "- `publisherClass`\n",
        "\n",
        "**Optional Columns**:\n",
        "- `firstReleaseDate` (used to compute release year if included)\n",
        "- `positive_ratio` (computed automatically if not provided)\n",
        "\n",
        "**Excluded**:\n",
        "- `copiesSold` — excluded to avoid data leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_mm2qZ_fBwfE"
      },
      "id": "_mm2qZ_fBwfE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "811d0cfe",
      "metadata": {
        "id": "811d0cfe"
      },
      "source": [
        "## 1) Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install xgboost"
      ],
      "metadata": {
        "id": "4OvsnVpBYLAB"
      },
      "id": "4OvsnVpBYLAB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8c9b30",
      "metadata": {
        "id": "ee8c9b30"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install Excel reader\n",
        "!pip -q install openpyxl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42301efc",
      "metadata": {
        "id": "42301efc"
      },
      "source": [
        "\n",
        "## 2) (Optional) Mount Google Drive\n",
        "\n",
        "If your file lives in Drive, keep this ON and update the `DRIVE_DATA_PATH` below.\n",
        "If not, we'll auto-fallback to a manual file upload in the next step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fc863a3",
      "metadata": {
        "id": "1fc863a3"
      },
      "outputs": [],
      "source": [
        "\n",
        "MOUNT_DRIVE = True  # Set to False if you DON'T use Drive\n",
        "DRIVE_DATA_PATH = \"/content/drive/MyDrive/private/GamesDataSet.xlsx\"  # <-- change if needed\n",
        "\n",
        "if MOUNT_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a613573",
      "metadata": {
        "id": "0a613573"
      },
      "source": [
        "\n",
        "## 3) Load your **private** Excel file (sheet: `paid pc games`)\n",
        "\n",
        "- If `DRIVE_DATA_PATH` exists, we'll use it.\n",
        "- Otherwise, you'll be prompted to **upload** your Excel file from your computer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d12bf9fe",
      "metadata": {
        "id": "d12bf9fe"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "SHEET_NAME = \"paid pc games\"  # do not change\n",
        "\n",
        "def load_data():\n",
        "    # Try Drive path first\n",
        "    if MOUNT_DRIVE:\n",
        "        try:\n",
        "            df = pd.read_excel(DRIVE_DATA_PATH, sheet_name=SHEET_NAME)\n",
        "            print(f\"Loaded from Drive: {DRIVE_DATA_PATH}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Drive read failed ({e}). Falling back to file upload...\")\n",
        "    # Fallback: upload\n",
        "    from google.colab import files\n",
        "    print(\"Please upload your Excel file now...\")\n",
        "    uploaded = files.upload()\n",
        "    assert len(uploaded) >= 1, \"No file uploaded.\"\n",
        "    file_name = list(uploaded.keys())[0]\n",
        "    df = pd.read_excel(file_name, sheet_name=SHEET_NAME)\n",
        "    print(f\"Loaded uploaded file: {file_name}\")\n",
        "    return df\n",
        "\n",
        "df_raw = load_data()\n",
        "df_raw.head(5)  # preview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91f63c2c",
      "metadata": {
        "id": "91f63c2c"
      },
      "source": [
        "\n",
        "## 4) Configure training\n",
        "\n",
        "- We **drop** `copiesSold`.\n",
        "- We compute `positive_ratio` = `total_positive` / `total_reviews`.\n",
        "- `release_year` is **OFF** by default (you can enable it later).\n",
        "- To make training fast, we cap rows at `MAX_ROWS` (set `0` to use all rows).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5d054e7",
      "metadata": {
        "id": "b5d054e7"
      },
      "outputs": [],
      "source": [
        "\n",
        "USE_RELEASE_YEAR = False   # Set True if you want to include release year\n",
        "MAX_ROWS = 20000           # Training row cap for speed (set 0 to use all rows)\n",
        "\n",
        "TARGET = \"revenue\"\n",
        "NUMERIC_BASE = [\n",
        "    \"price\", \"total_reviews\", \"total_positive\", \"total_negative\",\n",
        "    \"reviewScore\", \"peak_all_time\"\n",
        "]\n",
        "CATEGORICAL = [\"publisherClass\"]\n",
        "\n",
        "DROP_COLUMNS = [\"copiesSold\"]  # explicitly remove\n",
        "DATE_COLUMN = \"firstReleaseDate\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d11129be",
      "metadata": {
        "id": "d11129be"
      },
      "source": [
        "\n",
        "## 5) Preprocess & Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a9dd01c",
      "metadata": {
        "id": "1a9dd01c"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def safe_ratio(a, b):\n",
        "    a = a.astype(float)\n",
        "    b = b.astype(float).replace(0, np.nan)\n",
        "    return (a / b).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "df = df_raw.copy()\n",
        "\n",
        "# Drop unwanted columns\n",
        "for col in DROP_COLUMNS:\n",
        "    if col in df.columns:\n",
        "        df = df.drop(columns=[col])\n",
        "\n",
        "# Positive ratio\n",
        "if \"positive_ratio\" not in df.columns:\n",
        "    if (\"total_positive\" in df.columns) and (\"total_reviews\" in df.columns):\n",
        "        df[\"positive_ratio\"] = safe_ratio(df[\"total_positive\"], df[\"total_reviews\"])\n",
        "    else:\n",
        "        df[\"positive_ratio\"] = np.nan\n",
        "\n",
        "# Release year (optional)\n",
        "if USE_RELEASE_YEAR:\n",
        "    if DATE_COLUMN in df.columns:\n",
        "        df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN], errors=\"coerce\")\n",
        "        df[\"release_year\"] = df[DATE_COLUMN].dt.year\n",
        "    else:\n",
        "        df[\"release_year\"] = np.nan\n",
        "\n",
        "# Build feature list\n",
        "feature_cols = NUMERIC_BASE + [\"positive_ratio\"] + CATEGORICAL\n",
        "if USE_RELEASE_YEAR:\n",
        "    feature_cols += [\"release_year\"]\n",
        "\n",
        "# Keep only available columns\n",
        "feature_cols = [c for c in feature_cols if c in df.columns]\n",
        "\n",
        "# Drop rows with missing target\n",
        "df = df[~df[TARGET].isna()].copy()\n",
        "\n",
        "# Downsample for speed if needed\n",
        "if MAX_ROWS and MAX_ROWS > 0 and len(df) > MAX_ROWS:\n",
        "    df = df.sample(n=MAX_ROWS, random_state=42)\n",
        "\n",
        "print(\"Rows used for training:\", len(df))\n",
        "print(\"Features:\", feature_cols)\n",
        "df[feature_cols + [TARGET]].head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccb7e26e",
      "metadata": {
        "id": "ccb7e26e"
      },
      "source": [
        "\n",
        "## 6) Train & Evaluate (RandomForest)\n",
        "- Train/test split (80/20)\n",
        "- Metrics: R², MAE, RMSE, MAPE\n",
        "- Top feature importances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c09b36da",
      "metadata": {
        "id": "c09b36da"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = df[feature_cols].copy()\n",
        "y = df[TARGET].astype(float).copy()\n",
        "\n",
        "numeric_cols = [c for c in feature_cols if c not in CATEGORICAL]\n",
        "categorical_cols = [c for c in feature_cols if c in CATEGORICAL]\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", SimpleImputer(strategy=\"median\"), numeric_cols),\n",
        "        (\"cat\", Pipeline(steps=[\n",
        "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "        ]), categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=150, n_jobs=-1, random_state=42\n",
        ")\n",
        "\n",
        "pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", rf)])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "pipe.fit(X_train, y_train)\n",
        "preds = pipe.predict(X_test)\n",
        "\n",
        "import numpy as np  # only needed if not already imported above\n",
        "\n",
        "mae = mean_absolute_error(y_test, preds)\n",
        "try:\n",
        "    rmse = mean_squared_error(y_test, preds, squared=False)  # newer sklearn\n",
        "except TypeError:\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, preds))        # older sklearn\n",
        "r2 = r2_score(y_test, preds)\n",
        "\n",
        "REVENUE_THRESHOLD = 1000\n",
        "\n",
        "# Mask for reasonable values\n",
        "mask = y_test >= REVENUE_THRESHOLD\n",
        "\n",
        "# Filter y_test and preds\n",
        "y_test_filtered = y_test[mask]\n",
        "preds_filtered = preds[mask]\n",
        "\n",
        "# Calculate safe MAPE\n",
        "EPSILON = REVENUE_THRESHOLD\n",
        "mape = np.mean(\n",
        "    np.abs((y_test_filtered - preds_filtered) / np.maximum(np.abs(y_test_filtered), EPSILON))\n",
        ") * 100\n",
        "\n",
        "\n",
        "print(f\"R2  : {r2:.3f}\")\n",
        "print(f\"MAE : {mae:,.0f}\")\n",
        "print(f\"RMSE: {rmse:,.0f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")\n",
        "\n",
        "# Feature importances\n",
        "# Get processed feature names\n",
        "ohe = pipe.named_steps[\"prep\"].named_transformers_[\"cat\"].named_steps[\"onehot\"] if categorical_cols else None\n",
        "cat_feature_names = ohe.get_feature_names_out(categorical_cols).tolist() if ohe is not None else []\n",
        "feature_names = numeric_cols + cat_feature_names\n",
        "\n",
        "importances = pipe.named_steps[\"model\"].feature_importances_\n",
        "imp_series = pd.Series(importances, index=feature_names).sort_values(ascending=False).head(15)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "imp_series[::-1].plot.barh()\n",
        "plt.title(\"Top Feature Importances (RandomForest)\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "385b30fa",
      "metadata": {
        "id": "385b30fa"
      },
      "source": [
        "\n",
        "## 7) Save trained model (`revenue_model.pkl`)\n",
        "\n",
        "- Saves locally to `/content/revenue_model.pkl`.\n",
        "- If Drive is mounted, also saves to `/content/drive/MyDrive/revenue_model.pkl`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "xgb_raw = XGBRegressor(\n",
        "    n_estimators=600,\n",
        "    max_depth=8,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.0,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    tree_method=\"hist\",\n",
        "    eval_metric=\"rmse\",\n",
        ")\n",
        "\n",
        "xgb_raw_pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", xgb_raw)])\n",
        "\n",
        "# (Early stopping handling same as before; keep your try/except if you want)\n",
        "try:\n",
        "    from xgboost.callback import EarlyStopping\n",
        "    xgb_raw_pipe.fit(\n",
        "        X_train, y_train,\n",
        "        model__eval_set=[(X_test, y_test)],\n",
        "        model__callbacks=[EarlyStopping(rounds=50, save_best=True)],\n",
        "        model__verbose=False,\n",
        "    )\n",
        "except Exception:\n",
        "    try:\n",
        "        xgb_raw_pipe.fit(\n",
        "            X_train, y_train,\n",
        "            model__eval_set=[(X_test, y_test)],\n",
        "            model__early_stopping_rounds=50,\n",
        "            model__verbose=False,\n",
        "        )\n",
        "    except Exception:\n",
        "        xgb_raw_pipe.fit(X_train, y_train)\n",
        "\n",
        "xgb_raw_preds = xgb_raw_pipe.predict(X_test)\n"
      ],
      "metadata": {
        "id": "miTjL8DnYWof"
      },
      "id": "miTjL8DnYWof",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_of(y_true, y_pred, label):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    try:\n",
        "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mape = safe_mape(y_true, y_pred, eps=1000)\n",
        "    return {\"Model\": label, \"R2\": round(r2,3), \"MAE\": int(mae), \"RMSE\": int(rmse), \"MAPE%\": round(mape,2)}\n",
        "\n",
        "rows = []\n",
        "# RandomForest (from earlier cell)\n",
        "rows.append(metrics_of(y_test, preds, \"RandomForest\"))\n",
        "# XGBoost\n",
        "rows.append(metrics_of(y_test, xgb_preds, \"XGBoost\"))\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame(rows)\n"
      ],
      "metadata": {
        "id": "mBJRwbRYaJfU"
      },
      "id": "mBJRwbRYaJfU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === XGBoost (LOG-target) — distinct variable names so we don't overwrite RAW ===\n",
        "!pip -q install xgboost  # ensures xgboost is present\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def safe_mape(y_true, y_pred, eps=1000):\n",
        "    return np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), eps))) * 100\n",
        "\n",
        "# 1) Log-transform target\n",
        "y_train_log = np.log1p(np.clip(y_train, a_min=0, a_max=None))\n",
        "y_test_log  = np.log1p(np.clip(y_test,  a_min=0, a_max=None))\n",
        "\n",
        "# 2) Model (LOG) — separate object\n",
        "xgb_log = XGBRegressor(\n",
        "    n_estimators=1200,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.03,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.0,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    tree_method=\"hist\",\n",
        "    eval_metric=\"rmse\",   # evaluated on LOG scale\n",
        ")\n",
        "\n",
        "xgb_log_pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", xgb_log)])\n",
        "\n",
        "# 3) Fit (version-agnostic early stopping)\n",
        "try:\n",
        "    from xgboost.callback import EarlyStopping  # xgboost >= 2.0\n",
        "    xgb_log_pipe.fit(\n",
        "        X_train, y_train_log,\n",
        "        model__eval_set=[(X_test, y_test_log)],\n",
        "        model__callbacks=[EarlyStopping(rounds=100, save_best=True)],\n",
        "        model__verbose=False,\n",
        "    )\n",
        "except Exception:\n",
        "    try:\n",
        "        xgb_log_pipe.fit(\n",
        "            X_train, y_train_log,\n",
        "            model__eval_set=[(X_test, y_test_log)],\n",
        "            model__early_stopping_rounds=100,\n",
        "            model__verbose=False,\n",
        "        )\n",
        "    except Exception:\n",
        "        xgb_log_pipe.fit(X_train, y_train_log)\n",
        "\n",
        "# 4) Predict back on $ scale — keep separate name\n",
        "preds_log = xgb_log_pipe.predict(X_test)\n",
        "xgb_log_preds = np.expm1(preds_log)\n",
        "\n",
        "# 5) Metrics (on $ scale)\n",
        "mae = mean_absolute_error(y_test, xgb_log_preds)\n",
        "try:\n",
        "    rmse = mean_squared_error(y_test, xgb_log_preds, squared=False)\n",
        "except TypeError:\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, xgb_log_preds))\n",
        "r2  = r2_score(y_test, xgb_log_preds)\n",
        "\n",
        "mape = safe_mape(y_test, xgb_log_preds, eps=1000)\n",
        "mask = y_test >= 1000\n",
        "mape_filtered = safe_mape(y_test[mask], xgb_log_preds[mask], eps=1000)\n",
        "\n",
        "print(f\"[XGBoost LOG-target]  R2: {r2:.3f}  MAE: {mae:,.0f}  RMSE: {rmse:,.0f}  MAPE*: {mape:.2f}%  MAPE>=1k: {mape_filtered:.2f}%\")\n",
        "print(\"* MAPE uses eps=1000 to avoid tiny-denominator blowups\")\n",
        "\n",
        "# 6) Store results for later table\n",
        "xgb_log_results = {\n",
        "    \"Model\": \"XGBoost (log)\",\n",
        "    \"R2\": round(r2, 3),\n",
        "    \"MAE\": int(mae),\n",
        "    \"RMSE\": int(rmse),\n",
        "    \"MAPE%\": round(mape, 2),\n",
        "}\n"
      ],
      "metadata": {
        "id": "pN42S137d4aV"
      },
      "id": "pN42S137d4aV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def safe_mape(y_true, y_pred, eps=1000):\n",
        "    \"\"\"MAPE with floor eps to avoid division blowups on tiny values.\"\"\"\n",
        "    return np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), eps))) * 100\n",
        "\n",
        "# === Log transform target ===\n",
        "y_train_log = np.log1p(y_train)   # log(1+y) to handle 0 safely\n",
        "y_test_log  = np.log1p(y_test)\n",
        "\n",
        "rf_log = RandomForestRegressor(\n",
        "    n_estimators=150,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_log_pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", rf_log)])\n",
        "\n",
        "# Fit on log targets\n",
        "rf_log_pipe.fit(X_train, y_train_log)\n",
        "\n",
        "# Predict (log scale), then convert back\n",
        "log_preds = rf_log_pipe.predict(X_test)\n",
        "rf_log_preds = np.expm1(log_preds)  # inverse of log1p\n",
        "\n",
        "# Evaluate\n",
        "mae  = mean_absolute_error(y_test, rf_log_preds)\n",
        "try:\n",
        "    rmse = mean_squared_error(y_test, rf_log_preds, squared=False)\n",
        "except TypeError:\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, rf_log_preds))\n",
        "r2   = r2_score(y_test, rf_log_preds)\n",
        "mape = safe_mape(y_test, rf_log_preds, eps=1000)\n",
        "\n",
        "print(f\"[RF-log]  R2: {r2:.3f}  MAE: {mae:,.0f}  RMSE: {rmse:,.0f}  MAPE*: {mape:.2f}%\")\n",
        "print(\"* MAPE uses eps=1000 to avoid tiny-denominator blowups\")"
      ],
      "metadata": {
        "id": "KaH5FjgEyV9p"
      },
      "id": "KaH5FjgEyV9p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def safe_mape(y_true, y_pred, eps=1000):\n",
        "    return np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), eps))) * 100\n",
        "\n",
        "def rmse_of(y_true, y_pred):\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def metrics_of(y_true, y_pred, label):\n",
        "    return {\n",
        "        \"Model\": label,\n",
        "        \"R2\": round(r2_score(y_true, y_pred), 3),\n",
        "        \"MAE\": int(mean_absolute_error(y_true, y_pred)),\n",
        "        \"RMSE\": int(rmse_of(y_true, y_pred)),\n",
        "        \"MAPE%\": round(safe_mape(y_true, y_pred, eps=1000), 2),\n",
        "    }\n",
        "\n",
        "rows = []\n",
        "rows.append(metrics_of(y_test, preds,          \"RandomForest (raw)\"))\n",
        "rows.append(metrics_of(y_test, rf_log_preds,   \"RandomForest (log)\"))\n",
        "rows.append(metrics_of(y_test, xgb_raw_preds,  \"XGBoost (raw)\"))\n",
        "rows.append(metrics_of(y_test, xgb_log_preds,  \"XGBoost (log)\"))\n",
        "\n",
        "pd.DataFrame(rows)\n"
      ],
      "metadata": {
        "id": "WRbgHbOi0O7J"
      },
      "id": "WRbgHbOi0O7J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a65895ad",
      "metadata": {
        "id": "a65895ad"
      },
      "outputs": [],
      "source": [
        "\n",
        "import joblib, os\n",
        "\n",
        "MODEL_LOCAL_PATH = \"/content/revenue_model.pkl\"\n",
        "joblib.dump(pipe, MODEL_LOCAL_PATH)\n",
        "print(\"Saved:\", MODEL_LOCAL_PATH)\n",
        "\n",
        "if MOUNT_DRIVE:\n",
        "    MODEL_DRIVE_PATH = \"/content/drive/MyDrive/revenue_model.pkl\"\n",
        "    joblib.dump(pipe, MODEL_DRIVE_PATH)\n",
        "    print(\"Saved:\", MODEL_DRIVE_PATH)\n",
        "\n",
        "# Optional: download to your computer\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(MODEL_LOCAL_PATH)\n",
        "except Exception as e:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21f5e95a",
      "metadata": {
        "id": "21f5e95a"
      },
      "source": [
        "\n",
        "## 8) Use the model to predict on a **new** file\n",
        "\n",
        "- Provide a path to another Excel/CSV with the same columns.\n",
        "- Output will be saved as `predictions.csv` with a `predicted_revenue` column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1cf96a9",
      "metadata": {
        "id": "a1cf96a9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# === Set your input path here ===\n",
        "PREDICT_INPUT_PATH = None  # e.g. \"/content/drive/MyDrive/my_new_games.xlsx\"\n",
        "\n",
        "def load_any(path):\n",
        "    if path.lower().endswith(\".xlsx\") or path.lower().endswith(\".xls\"):\n",
        "        return pd.read_excel(path, sheet_name=SHEET_NAME if SHEET_NAME in pd.ExcelFile(path).sheet_names else 0)\n",
        "    else:\n",
        "        return pd.read_csv(path)\n",
        "\n",
        "if PREDICT_INPUT_PATH is None:\n",
        "    print(\"Set PREDICT_INPUT_PATH to your new file path and re-run this cell.\")\n",
        "else:\n",
        "    model = joblib.load(MODEL_LOCAL_PATH)\n",
        "    dfp = load_any(PREDICT_INPUT_PATH).copy()\n",
        "\n",
        "    # --- minimal preprocessing to align columns ---\n",
        "    DROP_COLUMNS = [\"copiesSold\"]\n",
        "    for col in DROP_COLUMNS:\n",
        "        if col in dfp.columns:\n",
        "            dfp = dfp.drop(columns=[col])\n",
        "\n",
        "    def safe_ratio(a, b):\n",
        "        a = a.astype(float)\n",
        "        b = b.astype(float).replace(0, np.nan)\n",
        "        return (a / b).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    if \"positive_ratio\" not in dfp.columns:\n",
        "        if (\"total_positive\" in dfp.columns) and (\"total_reviews\" in dfp.columns):\n",
        "            dfp[\"positive_ratio\"] = safe_ratio(dfp[\"total_positive\"], dfp[\"total_reviews\"])\n",
        "        else:\n",
        "            dfp[\"positive_ratio\"] = np.nan\n",
        "\n",
        "    # Only include columns the model was trained with\n",
        "    feature_cols = [c for c in model.named_steps[\"prep\"].transformers_[0][2] + model.named_steps[\"prep\"].transformers_[1][2]]\n",
        "    use_cols = [c for c in feature_cols if c in dfp.columns]\n",
        "\n",
        "    preds = model.predict(dfp[use_cols])\n",
        "    out = dfp.copy()\n",
        "    out[\"predicted_revenue\"] = preds\n",
        "\n",
        "    out_path = \"/content/predictions.csv\"\n",
        "    out.to_csv(out_path, index=False)\n",
        "    print(\"Saved predictions:\", out_path)\n",
        "\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(out_path)\n",
        "    except Exception as e:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7c0x7O29cpMX"
      },
      "id": "7c0x7O29cpMX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}